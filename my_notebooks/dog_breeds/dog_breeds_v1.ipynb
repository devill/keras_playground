{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9422 images belonging to 121 classes.\n",
      "Found 800 images belonging to 121 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = image.ImageDataGenerator().flow_from_directory(\n",
    "    '/data/dog_breeds/train', \n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical', \n",
    "    shuffle=True, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "valid_batches = image.ImageDataGenerator().flow_from_directory(\n",
    "    '/data/dog_breeds/valid', \n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical', \n",
    "    shuffle=True, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,3))\n",
    "def vgg_preprocess(x):\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr\n",
    "\n",
    "def ConvBlock(model, layers, filters):\n",
    "    for i in range(layers):\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Conv2D(filters, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "def FCBlock(model):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "def BuildVGG():\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(vgg_preprocess, input_shape=(224,224,3)))\n",
    "    ConvBlock(model, 2, 64)\n",
    "    ConvBlock(model, 2, 128)\n",
    "    ConvBlock(model, 3, 256)\n",
    "    ConvBlock(model, 3, 512)\n",
    "    ConvBlock(model, 3, 512)\n",
    "\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    model.load_weights('/data/trained_models/vgg16_tf.h5')\n",
    "    model.compile(optimizer=Nadam(lr=0.001),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_model = BuildVGG()\n",
    "cd_model.pop()\n",
    "cd_model.pop()\n",
    "cd_model.pop()\n",
    "cd_model.pop()\n",
    "cd_model.pop()\n",
    "cd_model.add(Dense(4096, activation='relu'))\n",
    "cd_model.add(BatchNormalization())\n",
    "cd_model.add(Dropout(0.3))\n",
    "cd_model.add(Dense(4096, activation='relu'))\n",
    "cd_model.add(BatchNormalization())\n",
    "cd_model.add(Dropout(0.3))\n",
    "cd_model.add(Dense(121, activation='softmax'))\n",
    "\n",
    "opt = Nadam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "cd_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of the flatten layer\n",
    "original_layers = cd_model.layers\n",
    "flatten = [x for x in original_layers if x.name == 'flatten'][0]\n",
    "flatten_idx = original_layers.index(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable_layers_after(n, model):\n",
    "    for i, layer in enumerate(cd_model.layers):\n",
    "        layer.trainable = i > n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9422 images belonging to 121 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches_augmented = image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ").flow_from_directory(\n",
    "    '/data/dog_breeds/train', \n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical', \n",
    "    shuffle=True, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_trainable_layers_after(flatten_idx, cd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1178/1177 [==============================] - 235s - loss: 5.7907 - acc: 0.0103 - val_loss: 5.6328 - val_acc: 0.0138\n",
      "Epoch 2/4\n",
      "1178/1177 [==============================] - 232s - loss: 5.6271 - acc: 0.0119 - val_loss: 15.7826 - val_acc: 0.0112\n",
      "Epoch 3/4\n",
      "1178/1177 [==============================] - 227s - loss: 5.5970 - acc: 0.0126 - val_loss: 8.8515 - val_acc: 0.0063\n",
      "Epoch 4/4\n",
      "1178/1177 [==============================] - 229s - loss: 5.5854 - acc: 0.0133 - val_loss: 5.3614 - val_acc: 0.0187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07cc206a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd_model.fit_generator(train_batches_augmented, \n",
    "                 steps_per_epoch=train_batches.samples/train_batches.batch_size,\n",
    "                 epochs=4, \n",
    "                 validation_data=valid_batches, \n",
    "                 validation_steps=valid_batches.samples/valid_batches.batch_size\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 5.5268 - acc: 0.0145 - val_loss: 6.7538 - val_acc: 0.0175\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 5.4958 - acc: 0.0158 - val_loss: 5.3313 - val_acc: 0.0325\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 5.4974 - acc: 0.0144 - val_loss: 5.3517 - val_acc: 0.0250\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 5.4704 - acc: 0.0140 - val_loss: 6.4834 - val_acc: 0.0163\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 5.4261 - acc: 0.0163 - val_loss: 5.8758 - val_acc: 0.0275\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 231s - loss: 5.4319 - acc: 0.0129 - val_loss: 5.7108 - val_acc: 0.0112\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 232s - loss: 5.3989 - acc: 0.0135 - val_loss: 5.2030 - val_acc: 0.0213\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 226s - loss: 5.3745 - acc: 0.0171 - val_loss: 6.3330 - val_acc: 0.0213\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 234s - loss: 5.3253 - acc: 0.0213 - val_loss: 5.3063 - val_acc: 0.0338\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 230s - loss: 5.2635 - acc: 0.0219 - val_loss: 5.5578 - val_acc: 0.0325\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 227s - loss: 5.2376 - acc: 0.0242 - val_loss: 5.1778 - val_acc: 0.0300\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 5.1758 - acc: 0.0262 - val_loss: 5.0773 - val_acc: 0.0338\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 5.1288 - acc: 0.0292 - val_loss: 4.7903 - val_acc: 0.0362\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 232s - loss: 5.0911 - acc: 0.0294 - val_loss: 4.9596 - val_acc: 0.0350\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 5.0256 - acc: 0.0302 - val_loss: 7.5462 - val_acc: 0.0150\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 227s - loss: 5.0088 - acc: 0.0376 - val_loss: 5.1415 - val_acc: 0.0362\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 4.9644 - acc: 0.0369 - val_loss: 5.4761 - val_acc: 0.0537\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 4.9082 - acc: 0.0380 - val_loss: 5.0732 - val_acc: 0.0500\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 4.8392 - acc: 0.0417 - val_loss: 4.9963 - val_acc: 0.0512\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 4.8099 - acc: 0.0400 - val_loss: 4.9702 - val_acc: 0.0537\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 4.7260 - acc: 0.0432 - val_loss: 4.9583 - val_acc: 0.0387\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 4.6734 - acc: 0.0529 - val_loss: 5.2406 - val_acc: 0.0587\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 4.6235 - acc: 0.0534 - val_loss: 4.7619 - val_acc: 0.0600\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 228s - loss: 4.5346 - acc: 0.0581 - val_loss: 4.7235 - val_acc: 0.0862\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 227s - loss: 4.4926 - acc: 0.0664 - val_loss: 4.6377 - val_acc: 0.0775\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 230s - loss: 4.4198 - acc: 0.0699 - val_loss: 4.6250 - val_acc: 0.0675\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 226s - loss: 4.3697 - acc: 0.0751 - val_loss: 5.3152 - val_acc: 0.0525\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 224s - loss: 4.3212 - acc: 0.0765 - val_loss: 4.3758 - val_acc: 0.0825\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 225s - loss: 4.2341 - acc: 0.0877 - val_loss: 4.6107 - val_acc: 0.0925\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 4.2237 - acc: 0.0859 - val_loss: 4.8537 - val_acc: 0.0912\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 230s - loss: 4.1259 - acc: 0.0992 - val_loss: 4.4093 - val_acc: 0.1150\n",
      "Epoch 1/1\n",
      "1178/1177 [==============================] - 229s - loss: 4.0623 - acc: 0.1031 - val_loss: 4.7224 - val_acc: 0.0762\n"
     ]
    }
   ],
   "source": [
    "for idx in range(flatten_idx-1, -1, -1):\n",
    "    set_trainable_layers_after(idx, cd_model)\n",
    "    cd_model.fit_generator(train_batches_augmented, \n",
    "                     steps_per_epoch=train_batches.samples/train_batches.batch_size,\n",
    "                     epochs=1, \n",
    "                     validation_data=valid_batches, \n",
    "                     validation_steps=valid_batches.samples/valid_batches.batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd_model.save_weights('/data/trained_models/dog_breeds_fulltrain_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
