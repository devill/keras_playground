{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook \n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/data/spooky_author/train.csv')\n",
    "test = pd.read_csv('/data/spooky_author/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svec = pd.read_csv(\"/data/spooky_author/train_sen2vec.dat\", header=None, delimiter=r\"\\s+\")\n",
    "test_svec = pd.read_csv(\"/data/spooky_author/test_sen2vec.dat\", header=None, delimiter=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053488</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>-0.019853</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>-0.162330</td>\n",
       "      <td>0.065901</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.046524</td>\n",
       "      <td>-0.003078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049284</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>-0.058413</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>0.127950</td>\n",
       "      <td>-0.19166</td>\n",
       "      <td>-0.009965</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.129180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037915</td>\n",
       "      <td>-0.120860</td>\n",
       "      <td>-0.072031</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>-0.424230</td>\n",
       "      <td>0.159270</td>\n",
       "      <td>-0.081277</td>\n",
       "      <td>0.067744</td>\n",
       "      <td>-0.301060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085751</td>\n",
       "      <td>-0.101140</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>-0.131190</td>\n",
       "      <td>0.212910</td>\n",
       "      <td>0.12213</td>\n",
       "      <td>0.015365</td>\n",
       "      <td>-0.041215</td>\n",
       "      <td>-0.073558</td>\n",
       "      <td>0.314440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075236</td>\n",
       "      <td>0.048786</td>\n",
       "      <td>0.028489</td>\n",
       "      <td>-0.094034</td>\n",
       "      <td>0.074649</td>\n",
       "      <td>-0.192110</td>\n",
       "      <td>0.106330</td>\n",
       "      <td>-0.044206</td>\n",
       "      <td>0.057636</td>\n",
       "      <td>-0.071370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094568</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.140590</td>\n",
       "      <td>0.052424</td>\n",
       "      <td>0.035876</td>\n",
       "      <td>-0.14266</td>\n",
       "      <td>-0.271390</td>\n",
       "      <td>-0.150620</td>\n",
       "      <td>-0.070459</td>\n",
       "      <td>0.028201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.102540</td>\n",
       "      <td>-0.024934</td>\n",
       "      <td>0.160980</td>\n",
       "      <td>-0.037602</td>\n",
       "      <td>-0.078291</td>\n",
       "      <td>-0.096557</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.054484</td>\n",
       "      <td>-0.273560</td>\n",
       "      <td>0.110370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093459</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>-0.199820</td>\n",
       "      <td>0.114150</td>\n",
       "      <td>-0.072864</td>\n",
       "      <td>0.04788</td>\n",
       "      <td>-0.140480</td>\n",
       "      <td>-0.043337</td>\n",
       "      <td>-0.100850</td>\n",
       "      <td>0.149470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130780</td>\n",
       "      <td>0.033440</td>\n",
       "      <td>-0.144580</td>\n",
       "      <td>-0.025771</td>\n",
       "      <td>-0.307520</td>\n",
       "      <td>-0.092067</td>\n",
       "      <td>-0.014837</td>\n",
       "      <td>0.030967</td>\n",
       "      <td>-0.057631</td>\n",
       "      <td>0.088422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091501</td>\n",
       "      <td>0.167880</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>-0.161370</td>\n",
       "      <td>-0.245880</td>\n",
       "      <td>-0.12596</td>\n",
       "      <td>-0.031658</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.027196</td>\n",
       "      <td>0.213870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.053488  0.067535 -0.019853  0.068202 -0.162330  0.065901  0.028737   \n",
       "1 -0.037915 -0.120860 -0.072031  0.270100  0.313400 -0.424230  0.159270   \n",
       "2  0.075236  0.048786  0.028489 -0.094034  0.074649 -0.192110  0.106330   \n",
       "3 -0.102540 -0.024934  0.160980 -0.037602 -0.078291 -0.096557  0.033061   \n",
       "4  0.130780  0.033440 -0.144580 -0.025771 -0.307520 -0.092067 -0.014837   \n",
       "\n",
       "        7         8         9      ...          590       591       592  \\\n",
       "0  0.166400  0.046524 -0.003078    ...     0.049284  0.161730 -0.058413   \n",
       "1 -0.081277  0.067744 -0.301060    ...    -0.085751 -0.101140  0.359500   \n",
       "2 -0.044206  0.057636 -0.071370    ...     0.094568  0.007301  0.140590   \n",
       "3  0.054484 -0.273560  0.110370    ...    -0.093459 -0.080855 -0.199820   \n",
       "4  0.030967 -0.057631  0.088422    ...    -0.091501  0.167880  0.119080   \n",
       "\n",
       "        593       594      595       596       597       598       599  \n",
       "0 -0.160580  0.127950 -0.19166 -0.009965  0.063620  0.017981  0.129180  \n",
       "1 -0.131190  0.212910  0.12213  0.015365 -0.041215 -0.073558  0.314440  \n",
       "2  0.052424  0.035876 -0.14266 -0.271390 -0.150620 -0.070459  0.028201  \n",
       "3  0.114150 -0.072864  0.04788 -0.140480 -0.043337 -0.100850  0.149470  \n",
       "4 -0.161370 -0.245880 -0.12596 -0.031658  0.031667  0.027196  0.213870  \n",
       "\n",
       "[5 rows x 600 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(multiclass_logloss_foreach(actual,predicted))\n",
    "    return vsota / rows\n",
    "\n",
    "def multiclass_logloss_foreach(actual,predicted,eps=1e-15):\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    return np.sum(-1.0 * actual * np.log(clip), axis=1)\n",
    "    \n",
    "\n",
    "def accuracy(actual,predicted):\n",
    "    if len(actual.shape) != 1:\n",
    "        actual = np.argmax(actual, axis=1)\n",
    "        \n",
    "    return np.sum(actual == np.argmax(predicted, axis=1))/actual.shape[0]\n",
    "\n",
    "def print_metrics(actual, predicted):\n",
    "    print (\"logloss: %0.3f, accuracy: %0.3f \" % (multiclass_logloss(actual, predicted), accuracy(actual, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "labels = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid, xvtrain, xvvalid = train_test_split(train.text.values, labels, train_svec.as_matrix(),\n",
    "                                                  stratify=labels, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "xtest = test.text.values\n",
    "xvtest = test_svec.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand crafted features\n",
    "\n",
    "def sentence2handfeatures(s):\n",
    "    return [\n",
    "        len(s),\n",
    "        len(s.split()),\n",
    "        len(set(s.split())),\n",
    "        len([w for w in str(s).lower().split() if w in stop_words]),\n",
    "        len([c for c in str(s) if c in string.punctuation]),\n",
    "        len([w for w in str(s).split() if w.isupper()]),\n",
    "        len([w for w in str(s).split() if w.istitle()]),\n",
    "        np.mean([len(w) for w in str(s).split()])\n",
    "    ]\n",
    "    \n",
    "def handfeatures(d):\n",
    "    return np.array(list(map(lambda s: sentence2handfeatures(s), d)))\n",
    "\n",
    "xtrain_hand = handfeatures(xtrain)\n",
    "xvalid_hand = handfeatures(xvalid)\n",
    "xtest_hand = handfeatures(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = None)\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "xtest_tfv = tfv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwtv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = None)\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "cwtv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_cwtv =  cwtv.transform(xtrain) \n",
    "xvalid_cwtv = cwtv.transform(xvalid)\n",
    "xtest_cwtv = cwtv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cctv = CountVectorizer(analyzer='char_wb',\n",
    "            ngram_range=(1, 3), stop_words = None)\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "cctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_cctv =  cctv.transform(xtrain) \n",
    "xvalid_cctv = cctv.transform(xvalid)\n",
    "xtest_cctv = cctv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 9236)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_cctv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<17621x9236 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3762200 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xtrain_cctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv = hstack((xtrain_tfv, xtrain_cwtv,xtrain_cctv,xtrain_hand))\n",
    "xvalid_cwtv_cctv = hstack((xvalid_tfv, xvalid_cwtv,xvalid_cctv,xvalid_hand))\n",
    "xtest_cwtv_cctv = hstack((xtest_tfv, xtest_cwtv,xtest_cctv, xtest_hand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1958, 713391)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_cwtv_cctv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.469, accuracy: 0.837 \n"
     ]
    }
   ],
   "source": [
    "comb_lf = LogisticRegression(C=1.0)\n",
    "comb_lf.fit(xtrain_cwtv_cctv, ytrain)\n",
    "predictions_comb = comb_lf.predict_proba(xvalid_cwtv_cctv)\n",
    "\n",
    "print_metrics(yvalid, predictions_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.383, accuracy: 0.858 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions_nbtfidf = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print_metrics(yvalid, predictions_nbtfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.360, accuracy: 0.866 \n"
     ]
    }
   ],
   "source": [
    "predictions_ensamble1 = (predictions_comb + predictions_nbtfidf)/2\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_comb = comb_lf.predict_proba(xtest_cwtv_cctv)\n",
    "test_predictions_nbtfidf = clf.predict_proba(xtest_tfv)\n",
    "test_predictions_ensamble1 = (predictions_comb + predictions_nbtfidf)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble1, columns=lbl_enc.classes_)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('/data/spooky_author/submition_1.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = multiclass_logloss_foreach(yvalid, predictions_ensamble1)\n",
    "sorted_by_loss = np.argsort(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138 Actual: HPL, Predicted: EAP, loss: 6.380 \n",
      "I repeat to you, gentlemen, that your inquisition is fruitless.\n",
      "\n",
      "1623 Actual: HPL, Predicted: EAP, loss: 5.214 \n",
      "In relating the circumstances which have led to my confinement within this refuge for the demented, I am aware that my present position will create a natural doubt of the authenticity of my narrative.\n",
      "\n",
      "154 Actual: MWS, Predicted: EAP, loss: 4.822 \n",
      "The Cenci The Brides' Tragedy, by T. L. Beddoes, Esq.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_classes = lbl_enc.classes_\n",
    "\n",
    "for i in range(3):\n",
    "    idx = sorted_by_loss[-(i+1)]\n",
    "    \n",
    "    print(\"%d Actual: %s, Predicted: %s, loss: %0.3f \" % (\n",
    "        idx,\n",
    "        author_classes[yvalid[idx]],\n",
    "        author_classes[np.argmax(predictions_ensamble1[idx])],\n",
    "        losses[idx]\n",
    "    ))\n",
    "    \n",
    "    print(xvalid[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195895 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/data/glove/glove.840B.300d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        \n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17621/17621 [00:04<00:00, 3717.17it/s]\n",
      "100%|██████████| 1958/1958 [00:00<00:00, 3762.93it/s]\n",
      "100%|██████████| 8392/8392 [00:02<00:00, 3710.29it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = np.array([sent2vec(x) for x in tqdm(xtrain)])\n",
    "xvalid_glove = np.array([sent2vec(x) for x in tqdm(xvalid)])\n",
    "xtest_glove = np.array([sent2vec(x) for x in tqdm(xtest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.739, accuracy: 0.718 \n"
     ]
    }
   ],
   "source": [
    "glove_lr = LogisticRegression(C=1.0)\n",
    "glove_lr.fit(xtrain_glove, ytrain)\n",
    "predictions_glove = glove_lr.predict_proba(xvalid_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv_glove = hstack((xtrain_glove,xtrain_cwtv,xtrain_cctv,xtrain_hand))\n",
    "xvalid_cwtv_cctv_glove = hstack((xvalid_glove,xvalid_cwtv,xvalid_cctv,xvalid_hand))\n",
    "xtest_cwtv_cctv_glove = hstack((xtest_glove,xtest_cwtv,xtest_cctv,xtest_hand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.466, accuracy: 0.841 \n"
     ]
    }
   ],
   "source": [
    "multi_lr_2 = LogisticRegression(C=1.0)\n",
    "multi_lr_2.fit(xtrain_cwtv_cctv_glove, ytrain)\n",
    "predictions_multi_2 = multi_lr_2.predict_proba(xvalid_cwtv_cctv_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_multi_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.358, accuracy: 0.869 \n"
     ]
    }
   ],
   "source": [
    "predictions_ensamble2 = (predictions_multi_2 + predictions_nbtfidf)/2\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit second model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_multi2 = multi_lr_2.predict_proba(xtest_cwtv_cctv_glove)\n",
    "test_predictions_ensamble2 = (test_predictions_multi2 + test_predictions_nbtfidf)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_2 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble2, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_2.to_csv('/data/spooky_author/submition_2.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network on existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 660063) (17621,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_cwtv_cctv_glove.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Dropout\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "dl_model_1 = Sequential([\n",
    "    Dense(100, input_shape=(xvalid_cwtv_cctv_glove.shape[1],)),\n",
    "    LeakyReLU(alpha=0.3),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Nadam()\n",
    "dl_model_1.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               66006400  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 66,006,703\n",
      "Trainable params: 66,006,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dl_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_from_sparse_array(x,y, batch_size=64):\n",
    "    xcsr = x.tocsr()\n",
    "    while True:\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            yield (xcsr[i:i+batch_size].todense(), y[i:i+batch_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.7555 - acc: 0.7128 - val_loss: 0.4303 - val_acc: 0.8345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6a062442b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_gen = batches_from_sparse_array(xtrain_cwtv_cctv_glove, ytrain, batch_size=batch_size)\n",
    "valid_gen = batches_from_sparse_array(xvalid_cwtv_cctv_glove, yvalid, batch_size=batch_size)\n",
    "\n",
    "steps = math.ceil(ytrain.shape[0]/batch_size)\n",
    "valid_steps = math.ceil(yvalid.shape[0]/batch_size)\n",
    "\n",
    "dl_model_1.fit_generator(\n",
    "    train_gen, \n",
    "    steps_per_epoch=steps,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps = valid_steps,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.430, accuracy: 0.835 \n"
     ]
    }
   ],
   "source": [
    "valid_gen = batches_from_sparse_array(xvalid_cwtv_cctv_glove, yvalid, batch_size=batch_size)\n",
    "steps = math.ceil(xvalid_cwtv_cctv_glove.shape[0]/batch_size)\n",
    "valid_predictions_dl_1 = dl_model_1.predict_generator(valid_gen, steps=steps)\n",
    "\n",
    "print_metrics(yvalid, valid_predictions_dl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.41999999999999998, 0.0, 0.58000000000000007)\n",
      "logloss: 0.359, accuracy: 0.868 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0)\n",
    "for i in tqdm(np.arange(0,1,0.02)):\n",
    "    for j in np.arange(0,1-i,0.02):\n",
    "        for k in np.arange(0,1-i-j,0.02):\n",
    "            predictions_ensamble2 = (\n",
    "                i*valid_predictions_dl_1 + \n",
    "                j*predictions_multi_2 + \n",
    "                k*predictions_comb + \n",
    "                (1-i-j-k)*predictions_nbtfidf\n",
    "            )\n",
    "            \n",
    "            loss = multiclass_logloss(yvalid, predictions_ensamble2)\n",
    "            if best_loss > loss: \n",
    "                best_loss = loss\n",
    "                best_params = (i,j,k,1-i-j-k)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble2 = (\n",
    "                best_params[0]*valid_predictions_dl_1 + \n",
    "                best_params[1]*predictions_multi_2 + \n",
    "                best_params[2]*predictions_comb + \n",
    "                best_params[3]*predictions_nbtfidf\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = batches_from_sparse_array(xtest_cwtv_cctv_glove, np.zeros(xtest_cwtv_cctv_glove.shape[0]), batch_size=batch_size)\n",
    "steps = math.ceil(xtest_cwtv_cctv_glove.shape[0]/batch_size)\n",
    "test_predictions_dl_1 = dl_model_1.predict_generator(test_gen, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble2 = (\n",
    "                best_params[0]*test_predictions_dl_1 + \n",
    "                best_params[1]*test_predictions_multi2 + \n",
    "                best_params[2]*test_predictions_comb + \n",
    "                best_params[3]*test_predictions_nbtfidf\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_3 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble2, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_3.to_csv('/data/spooky_author/submition_3.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence to vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.778, accuracy: 0.683 \n"
     ]
    }
   ],
   "source": [
    "sen2vec_lr = LogisticRegression(C=1.0)\n",
    "sen2vec_lr.fit(xvtrain, ytrain)\n",
    "predictions_sen2vec = sen2vec_lr.predict_proba(xvvalid)\n",
    "\n",
    "print_metrics(yvalid, predictions_sen2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv_s2v_glove = hstack((xtrain_glove,xvtrain,xtrain_cwtv,xtrain_cctv,xtrain_hand))\n",
    "xvalid_cwtv_cctv_s2v_glove = hstack((xvalid_glove,xvvalid,xvalid_cwtv,xvalid_cctv,xvalid_hand))\n",
    "xtest_cwtv_cctv_s2v_glove = hstack((xtest_glove,xvtest,xtest_cwtv,xtest_cctv,xtest_hand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.477, accuracy: 0.828 \n"
     ]
    }
   ],
   "source": [
    "multi_lr_3 = LogisticRegression(C=1.0)\n",
    "multi_lr_3.fit(xtrain_cwtv_cctv_s2v_glove, ytrain)\n",
    "predictions_multi_3 = multi_lr_3.predict_proba(xvalid_cwtv_cctv_s2v_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_multi_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.25, 0.0, 0.55000000000000004, 0.19999999999999996)\n",
      "logloss: 0.357, accuracy: 0.868 \n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0)\n",
    "step = 0.05\n",
    "for i in np.arange(0,0.1,step):\n",
    "    for j in np.arange(0,1-i,step):\n",
    "        for k in np.arange(0,1-i-j,step):\n",
    "            for l in np.arange(0,1-i-j-k, step):\n",
    "                predictions_ensamble3 = (\n",
    "                    i*valid_predictions_dl_1 + \n",
    "                    j*predictions_multi_2 + \n",
    "                    k*predictions_comb + \n",
    "                    l*predictions_nbtfidf + \n",
    "                    (1-i-j-k-l)*predictions_multi_3\n",
    "                )\n",
    "\n",
    "                loss = multiclass_logloss(yvalid, predictions_ensamble3)\n",
    "                if best_loss > loss: \n",
    "                    best_loss = loss\n",
    "                    best_params = (i,j,k,l,1-i-j-k-l)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble3 = (\n",
    "                best_params[0]*valid_predictions_dl_1 + \n",
    "                best_params[1]*predictions_multi_2 + \n",
    "                best_params[2]*predictions_comb + \n",
    "                best_params[3]*predictions_nbtfidf +\n",
    "                best_params[4]*predictions_multi_3\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit fourth model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_multi_3 = multi_lr_3.predict_proba(xtest_cwtv_cctv_s2v_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble3 = (\n",
    "                best_params[0]*test_predictions_dl_1 + \n",
    "                best_params[1]*test_predictions_multi2 + \n",
    "                best_params[2]*test_predictions_comb + \n",
    "                best_params[3]*test_predictions_nbtfidf + \n",
    "                best_params[4]*test_predictions_multi_3\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_4 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble3, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_4.to_csv('/data/spooky_author/submition_4.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyvalid = np.argmax(predictions_ensamble2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[701,  31,  58],\n",
       "       [ 58, 481,  25],\n",
       "       [ 56,  28, 520]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(yvalid,pyvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f69597b9b00>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyPUQ4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a064b3c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(confusion_matrix(yvalid,pyvalid), cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lens = list(map(lambda x: len(x),xvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69fc491f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = plt.hist(line_lens, bins=10, range=(0, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_for_len_groups(preds,keys = [30,50,100,150,200,250,300,301]):\n",
    "    \n",
    "\n",
    "    ygrouped = {k:[] for k in keys}\n",
    "    pred_grouped = {k:[] for k in keys}\n",
    "\n",
    "    def get_bin(v):\n",
    "        for i in keys:\n",
    "            if i > v:\n",
    "                return i\n",
    "        return keys[-1]\n",
    "\n",
    "    for i in range(len(yvalid)):\n",
    "        b = get_bin(line_lens[i])\n",
    "        ygrouped[b].append(yvalid[i])\n",
    "        pred_grouped[b].append(preds[i])\n",
    "\n",
    "    for k in keys:\n",
    "        print('Less than', k, 'characters', end=' > ')\n",
    "        print_metrics(np.array(ygrouped[k]),np.array(pred_grouped[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less than 30 characters > logloss: 0.605, accuracy: 0.717 \n",
      "Less than 50 characters > logloss: 0.611, accuracy: 0.790 \n",
      "Less than 100 characters > logloss: 0.483, accuracy: 0.810 \n",
      "Less than 150 characters > logloss: 0.353, accuracy: 0.876 \n",
      "Less than 200 characters > logloss: 0.253, accuracy: 0.920 \n",
      "Less than 250 characters > logloss: 0.283, accuracy: 0.900 \n",
      "Less than 300 characters > logloss: 0.176, accuracy: 0.942 \n",
      "Less than 301 characters > logloss: 0.152, accuracy: 0.943 \n"
     ]
    }
   ],
   "source": [
    "print_metrics_for_len_groups(predictions_ensamble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less than 150 characters > logloss: 0.448, accuracy: 0.832 \n",
      "Less than 151 characters > logloss: 0.231, accuracy: 0.922 \n"
     ]
    }
   ],
   "source": [
    "print_metrics_for_len_groups(predictions_ensamble2, [150,151])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf weighted glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfvs = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1,1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfvs.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfvs =  tfvs.transform(xtrain) \n",
    "xvalid_tfvs = tfvs.transform(xvalid)\n",
    "xtest_tfvs = tfvs.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvs_vocab = tfvs.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13003"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfvs.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec_tfidf(s,tfidf):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        if w in tfvs_vocab:\n",
    "            weight = tfidf[0,tfvs_vocab[w]]\n",
    "        else:\n",
    "            weight = 0.01\n",
    "        try:\n",
    "            M.append(embeddings_index[w]*weight)\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    l = np.sqrt((v ** 2).sum())\n",
    "    \n",
    "    if type(v) != np.ndarray or l < 0.0000001:\n",
    "        return np.zeros(300)\n",
    "    return v / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_weighted_glove = np.array([sent2vec_tfidf(x,xtrain_tfvs[i]) for i,x in enumerate(xtrain)])\n",
    "xvalid_weighted_glove = np.array([sent2vec_tfidf(x,xvalid_tfvs[i]) for i,x in enumerate(xvalid)])\n",
    "xtest_weighted_glove = np.array([sent2vec_tfidf(x,xtest_tfvs[i]) for i,x in enumerate(xtest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.760, accuracy: 0.686 \n"
     ]
    }
   ],
   "source": [
    "wglove_lr = LogisticRegression(C=1.0)\n",
    "wglove_lr.fit(xtrain_weighted_glove, ytrain)\n",
    "predictions_wglove = wglove_lr.predict_proba(xvalid_weighted_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_wglove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv_s2v_wglove = hstack((xtrain_weighted_glove,xtrain_glove,xvtrain,xtrain_cwtv,xtrain_cctv,xtrain_hand))\n",
    "xvalid_cwtv_cctv_s2v_wglove = hstack((xvalid_weighted_glove,xvalid_glove,xvvalid,xvalid_cwtv,xvalid_cctv,xvalid_hand))\n",
    "xtest_cwtv_cctv_s2v_wglove = hstack((xtest_weighted_glove,xtest_glove,xvtest,xtest_cwtv,xtest_cctv,xtest_hand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.469, accuracy: 0.836 \n"
     ]
    }
   ],
   "source": [
    "multi_lr_4 = LogisticRegression(C=1.0)\n",
    "multi_lr_4.fit(xtrain_cwtv_cctv_s2v_wglove, ytrain)\n",
    "predictions_multi_4 = multi_lr_4.predict_proba(xvalid_cwtv_cctv_s2v_wglove)\n",
    "\n",
    "print_metrics(yvalid, predictions_multi_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.10000000000000001, 0.0, 0.60000000000000009, 0.0, 0.29999999999999993)\n",
      "logloss: 0.355, accuracy: 0.871 \n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0)\n",
    "step = 0.1\n",
    "for i in np.arange(0,1,step):\n",
    "    for j in np.arange(0,1-i,step):\n",
    "        for k in np.arange(0,1-i-j,step):\n",
    "            for l in np.arange(0,1-i-j-k, step):\n",
    "                for m in np.arange(0,1-i-j-k-l, step):\n",
    "                    predictions_ensamble3 = (\n",
    "                        i*valid_predictions_dl_1 + \n",
    "                        j*predictions_multi_2 + \n",
    "                        k*predictions_comb + \n",
    "                        l*predictions_nbtfidf + \n",
    "                        m*predictions_multi_3 + \n",
    "                        (1-i-j-k-l-m)*predictions_multi_4\n",
    "                    )\n",
    "\n",
    "                    loss = multiclass_logloss(yvalid, predictions_ensamble3)\n",
    "                    if best_loss > loss: \n",
    "                        best_loss = loss\n",
    "                        best_params = (i,j,k,l,m,1-i-j-k-l-m)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble4 = (\n",
    "                best_params[0]*valid_predictions_dl_1 + \n",
    "                best_params[1]*predictions_multi_2 + \n",
    "                best_params[2]*predictions_comb + \n",
    "                best_params[3]*predictions_nbtfidf +\n",
    "                best_params[4]*predictions_multi_3 +\n",
    "                best_params[5]*predictions_multi_4\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit fifth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_multi_4 = multi_lr_4.predict_proba(xtest_cwtv_cctv_s2v_wglove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble4 = (\n",
    "                best_params[0]*test_predictions_dl_1 + \n",
    "                best_params[1]*test_predictions_multi2 + \n",
    "                best_params[2]*test_predictions_comb + \n",
    "                best_params[3]*test_predictions_nbtfidf + \n",
    "                best_params[4]*test_predictions_multi_3 + \n",
    "                best_params[5]*test_predictions_multi_4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_5 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble4, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_5.to_csv('/data/spooky_author/submition_5.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy good idea from kaggle\n",
    "https://www.kaggle.com/marcospinaci/0-335-log-loss-in-a-dozen-lines/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.354, accuracy: 0.869 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "models = [('MultiNB', MultinomialNB(alpha=0.03)),\n",
    "          ('Calibrated MultiNB', CalibratedClassifierCV(\n",
    "              MultinomialNB(alpha=0.03), method='isotonic')),\n",
    "          ('Calibrated BernoulliNB', CalibratedClassifierCV(\n",
    "              BernoulliNB(alpha=0.03), method='isotonic')),\n",
    "          ('Calibrated Huber', CalibratedClassifierCV(\n",
    "              SGDClassifier(loss='modified_huber', alpha=1e-4,\n",
    "                            max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "          ('Logit', LogisticRegression(C=30))]\n",
    "\n",
    "\n",
    "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
    "clf = VotingClassifier(models, voting='soft', weights=[3,3,3,1,1])\n",
    "\n",
    "vectorizer.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "xtrain_tfv2 = vectorizer.transform(xtrain)\n",
    "xvalid_tfv2 = vectorizer.transform(xvalid)\n",
    "xtest_tfv2 = vectorizer.transform(xtest)\n",
    "\n",
    "\n",
    "clf.fit(xtrain_tfv2, ytrain)\n",
    "predictions_tfv2 = clf.predict_proba(xvalid_tfv2)\n",
    "\n",
    "print_metrics(yvalid, predictions_tfv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, 0.0, 0.70000000000000007, 0.29999999999999993)\n",
      "logloss: 0.342, accuracy: 0.874 \n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0,0)\n",
    "step = 0.1\n",
    "for i in np.arange(0,1+step,step):\n",
    "    for j in np.arange(0,1-i+step,step):\n",
    "        for k in np.arange(0,1-i-j+step,step):\n",
    "            for l in np.arange(0,1-i-j-k+step, step):\n",
    "                predictions_ensamble5 = (\n",
    "                    i*predictions_multi_2 + \n",
    "                    j*predictions_nbtfidf + \n",
    "                    k*predictions_multi_3 + \n",
    "                    l*predictions_tfv2 +\n",
    "                    (1-i-j-k-l)*predictions_multi_4\n",
    "                )\n",
    "\n",
    "                loss = multiclass_logloss(yvalid, predictions_ensamble5)\n",
    "                if best_loss > loss: \n",
    "                    best_loss = loss\n",
    "                    best_params = (i,j,k,l,1-i-j-k-l)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble5 = (\n",
    "                best_params[0]*predictions_multi_2 + \n",
    "                best_params[1]*predictions_nbtfidf +\n",
    "                best_params[2]*predictions_multi_3 +\n",
    "                best_params[3]*predictions_tfv2 +\n",
    "                best_params[4]*predictions_multi_4\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit sixth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-13b6cee737ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_predictions_tfv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest_tfv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_log_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# normalize by P(x) = P(f_1, ..., f_n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mlog_prob_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    726\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "test_predictions_tfv2 = clf.predict_proba(xtest_tfv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble5 = (\n",
    "                best_params[0]*test_predictions_multi2 + \n",
    "                best_params[1]*test_predictions_nbtfidf +\n",
    "                best_params[2]*test_predictions_multi_3 +\n",
    "                best_params[3]*test_predictions_tfv2 +\n",
    "                best_params[4]*test_predictions_multi_4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_6 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble5, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_6.to_csv('/data/spooky_author/submition_6c.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "- boost ensamble\n",
    "- attention model\n",
    "- conv network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
