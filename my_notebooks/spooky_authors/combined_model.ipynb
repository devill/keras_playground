{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook \n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/data/spooky_author/train.csv')\n",
    "test = pd.read_csv('/data/spooky_author/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svec = pd.read_csv(\"/data/spooky_author/train_sen2vec.dat\", header=None, delimiter=r\"\\s+\")\n",
    "test_svec = pd.read_csv(\"/data/spooky_author/test_sen2vec.dat\", header=None, delimiter=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053488</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>-0.019853</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>-0.162330</td>\n",
       "      <td>0.065901</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.046524</td>\n",
       "      <td>-0.003078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049284</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>-0.058413</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>0.127950</td>\n",
       "      <td>-0.19166</td>\n",
       "      <td>-0.009965</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.129180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037915</td>\n",
       "      <td>-0.120860</td>\n",
       "      <td>-0.072031</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>-0.424230</td>\n",
       "      <td>0.159270</td>\n",
       "      <td>-0.081277</td>\n",
       "      <td>0.067744</td>\n",
       "      <td>-0.301060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085751</td>\n",
       "      <td>-0.101140</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>-0.131190</td>\n",
       "      <td>0.212910</td>\n",
       "      <td>0.12213</td>\n",
       "      <td>0.015365</td>\n",
       "      <td>-0.041215</td>\n",
       "      <td>-0.073558</td>\n",
       "      <td>0.314440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075236</td>\n",
       "      <td>0.048786</td>\n",
       "      <td>0.028489</td>\n",
       "      <td>-0.094034</td>\n",
       "      <td>0.074649</td>\n",
       "      <td>-0.192110</td>\n",
       "      <td>0.106330</td>\n",
       "      <td>-0.044206</td>\n",
       "      <td>0.057636</td>\n",
       "      <td>-0.071370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094568</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.140590</td>\n",
       "      <td>0.052424</td>\n",
       "      <td>0.035876</td>\n",
       "      <td>-0.14266</td>\n",
       "      <td>-0.271390</td>\n",
       "      <td>-0.150620</td>\n",
       "      <td>-0.070459</td>\n",
       "      <td>0.028201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.102540</td>\n",
       "      <td>-0.024934</td>\n",
       "      <td>0.160980</td>\n",
       "      <td>-0.037602</td>\n",
       "      <td>-0.078291</td>\n",
       "      <td>-0.096557</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.054484</td>\n",
       "      <td>-0.273560</td>\n",
       "      <td>0.110370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093459</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>-0.199820</td>\n",
       "      <td>0.114150</td>\n",
       "      <td>-0.072864</td>\n",
       "      <td>0.04788</td>\n",
       "      <td>-0.140480</td>\n",
       "      <td>-0.043337</td>\n",
       "      <td>-0.100850</td>\n",
       "      <td>0.149470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130780</td>\n",
       "      <td>0.033440</td>\n",
       "      <td>-0.144580</td>\n",
       "      <td>-0.025771</td>\n",
       "      <td>-0.307520</td>\n",
       "      <td>-0.092067</td>\n",
       "      <td>-0.014837</td>\n",
       "      <td>0.030967</td>\n",
       "      <td>-0.057631</td>\n",
       "      <td>0.088422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091501</td>\n",
       "      <td>0.167880</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>-0.161370</td>\n",
       "      <td>-0.245880</td>\n",
       "      <td>-0.12596</td>\n",
       "      <td>-0.031658</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.027196</td>\n",
       "      <td>0.213870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.053488  0.067535 -0.019853  0.068202 -0.162330  0.065901  0.028737   \n",
       "1 -0.037915 -0.120860 -0.072031  0.270100  0.313400 -0.424230  0.159270   \n",
       "2  0.075236  0.048786  0.028489 -0.094034  0.074649 -0.192110  0.106330   \n",
       "3 -0.102540 -0.024934  0.160980 -0.037602 -0.078291 -0.096557  0.033061   \n",
       "4  0.130780  0.033440 -0.144580 -0.025771 -0.307520 -0.092067 -0.014837   \n",
       "\n",
       "        7         8         9      ...          590       591       592  \\\n",
       "0  0.166400  0.046524 -0.003078    ...     0.049284  0.161730 -0.058413   \n",
       "1 -0.081277  0.067744 -0.301060    ...    -0.085751 -0.101140  0.359500   \n",
       "2 -0.044206  0.057636 -0.071370    ...     0.094568  0.007301  0.140590   \n",
       "3  0.054484 -0.273560  0.110370    ...    -0.093459 -0.080855 -0.199820   \n",
       "4  0.030967 -0.057631  0.088422    ...    -0.091501  0.167880  0.119080   \n",
       "\n",
       "        593       594      595       596       597       598       599  \n",
       "0 -0.160580  0.127950 -0.19166 -0.009965  0.063620  0.017981  0.129180  \n",
       "1 -0.131190  0.212910  0.12213  0.015365 -0.041215 -0.073558  0.314440  \n",
       "2  0.052424  0.035876 -0.14266 -0.271390 -0.150620 -0.070459  0.028201  \n",
       "3  0.114150 -0.072864  0.04788 -0.140480 -0.043337 -0.100850  0.149470  \n",
       "4 -0.161370 -0.245880 -0.12596 -0.031658  0.031667  0.027196  0.213870  \n",
       "\n",
       "[5 rows x 600 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(multiclass_logloss_foreach(actual,predicted))\n",
    "    return vsota / rows\n",
    "\n",
    "def multiclass_logloss_foreach(actual,predicted,eps=1e-15):\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    return np.sum(-1.0 * actual * np.log(clip), axis=1)\n",
    "    \n",
    "\n",
    "def accuracy(actual,predicted):\n",
    "    if len(actual.shape) != 1:\n",
    "        actual = np.argmax(actual, axis=1)\n",
    "        \n",
    "    return np.sum(actual == np.argmax(predicted, axis=1))/actual.shape[0]\n",
    "\n",
    "def print_metrics(actual, predicted):\n",
    "    print (\"logloss: %0.3f, accuracy: %0.3f \" % (multiclass_logloss(actual, predicted), accuracy(actual, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "labels = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid, xvtrain, xvvalid = train_test_split(train.text.values, labels, train_svec.as_matrix(),\n",
    "                                                  stratify=labels, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "xtest = test.text.values\n",
    "xvtest = test_svec.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "xtest_tfv = tfv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwtv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "cwtv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_cwtv =  cwtv.transform(xtrain) \n",
    "xvalid_cwtv = cwtv.transform(xvalid)\n",
    "xtest_cwtv = cwtv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cctv = CountVectorizer(analyzer='char_wb',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "cctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_cctv =  cctv.transform(xtrain) \n",
    "xvalid_cctv = cctv.transform(xvalid)\n",
    "xtest_cctv = cctv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 9236)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_cctv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<17621x9236 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3762200 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xtrain_cctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv = hstack((xtrain_tfv, xtrain_cwtv,xtrain_cctv))\n",
    "xvalid_cwtv_cctv = hstack((xvalid_tfv, xvalid_cwtv,xvalid_cctv))\n",
    "xtest_cwtv_cctv = hstack((xtest_tfv, xtest_cwtv,xtest_cctv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1958, 424604)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_cwtv_cctv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.495, accuracy: 0.818 \n"
     ]
    }
   ],
   "source": [
    "comb_lf = LogisticRegression(C=1.0)\n",
    "comb_lf.fit(xtrain_cwtv_cctv, ytrain)\n",
    "predictions_comb = comb_lf.predict_proba(xvalid_cwtv_cctv)\n",
    "\n",
    "print_metrics(yvalid, predictions_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.440, accuracy: 0.826 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions_nbtfidf = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print_metrics(yvalid, predictions_nbtfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.392, accuracy: 0.854 \n"
     ]
    }
   ],
   "source": [
    "predictions_ensamble1 = (predictions_comb + predictions_nbtfidf)/2\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_comb = comb_lf.predict_proba(xtest_cwtv_cctv)\n",
    "test_predictions_nbtfidf = clf.predict_proba(xtest_tfv)\n",
    "test_predictions_ensamble1 = (predictions_comb + predictions_nbtfidf)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble1, columns=lbl_enc.classes_)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('/data/spooky_author/submition_1.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = multiclass_logloss_foreach(yvalid, predictions_ensamble1)\n",
    "sorted_by_loss = np.argsort(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 Actual: MWS, Predicted: EAP, loss: 5.201 \n",
      "The Cenci The Brides' Tragedy, by T. L. Beddoes, Esq.\n",
      "\n",
      "1138 Actual: HPL, Predicted: EAP, loss: 5.020 \n",
      "I repeat to you, gentlemen, that your inquisition is fruitless.\n",
      "\n",
      "503 Actual: MWS, Predicted: HPL, loss: 3.953 \n",
      "The windows of the room had before been darkened, and I felt a kind of panic on seeing the pale yellow light of the moon illuminate the chamber.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_classes = lbl_enc.classes_\n",
    "\n",
    "for i in range(3):\n",
    "    idx = sorted_by_loss[-(i+1)]\n",
    "    \n",
    "    print(\"%d Actual: %s, Predicted: %s, loss: %0.3f \" % (\n",
    "        idx,\n",
    "        author_classes[yvalid[idx]],\n",
    "        author_classes[np.argmax(predictions_ensamble1[idx])],\n",
    "        losses[idx]\n",
    "    ))\n",
    "    \n",
    "    print(xvalid[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195895 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/data/glove/glove.840B.300d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        \n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17621/17621 [00:05<00:00, 3459.62it/s]\n",
      "100%|██████████| 1958/1958 [00:00<00:00, 3350.51it/s]\n",
      "100%|██████████| 8392/8392 [00:02<00:00, 3524.15it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = np.array([sent2vec(x) for x in tqdm(xtrain)])\n",
    "xvalid_glove = np.array([sent2vec(x) for x in tqdm(xvalid)])\n",
    "xtest_glove = np.array([sent2vec(x) for x in tqdm(xtest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.732, accuracy: 0.715 \n"
     ]
    }
   ],
   "source": [
    "glove_lr = LogisticRegression(C=1.0)\n",
    "glove_lr.fit(xtrain_glove, ytrain)\n",
    "predictions_glove = glove_lr.predict_proba(xvalid_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv_glove = hstack((xtrain_glove,xtrain_cwtv,xtrain_cctv))\n",
    "xvalid_cwtv_cctv_glove = hstack((xvalid_glove,xvalid_cwtv,xvalid_cctv))\n",
    "xtest_cwtv_cctv_glove = hstack((xtest_glove,xtest_cwtv,xtest_cctv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.480, accuracy: 0.829 \n"
     ]
    }
   ],
   "source": [
    "multi_lr_2 = LogisticRegression(C=1.0)\n",
    "multi_lr_2.fit(xtrain_cwtv_cctv_glove, ytrain)\n",
    "predictions_multi_2 = multi_lr_2.predict_proba(xvalid_cwtv_cctv_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_multi_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.387, accuracy: 0.863 \n"
     ]
    }
   ],
   "source": [
    "predictions_ensamble2 = (predictions_multi_2 + predictions_nbtfidf)/2\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit second model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_multi2 = multi_lr_2.predict_proba(xtest_cwtv_cctv_glove)\n",
    "test_predictions_ensamble2 = (test_predictions_multi2 + test_predictions_nbtfidf)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_2 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble2, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_2.to_csv('/data/spooky_author/submition_2.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network on existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 409802) (17621,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_cwtv_cctv_glove.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Dropout\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "dl_model_1 = Sequential([\n",
    "    Dense(100, input_shape=(xvalid_cwtv_cctv_glove.shape[1],)),\n",
    "    LeakyReLU(alpha=0.3),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Nadam()\n",
    "dl_model_1.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               40980300  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 40,980,603\n",
      "Trainable params: 40,980,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dl_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_from_sparse_array(x,y, batch_size=64):\n",
    "    xcsr = x.tocsr()\n",
    "    while True:\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            yield (xcsr[i:i+batch_size].todense(), y[i:i+batch_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "69/69 [==============================] - 38s 550ms/step - loss: 0.8187 - acc: 0.6847 - val_loss: 0.4626 - val_acc: 0.8192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ca634a240>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_gen = batches_from_sparse_array(xtrain_cwtv_cctv_glove, ytrain, batch_size=batch_size)\n",
    "valid_gen = batches_from_sparse_array(xvalid_cwtv_cctv_glove, yvalid, batch_size=batch_size)\n",
    "\n",
    "steps = math.ceil(ytrain.shape[0]/batch_size)\n",
    "valid_steps = math.ceil(yvalid.shape[0]/batch_size)\n",
    "\n",
    "dl_model_1.fit_generator(\n",
    "    train_gen, \n",
    "    steps_per_epoch=steps,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps = valid_steps,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.463, accuracy: 0.819 \n"
     ]
    }
   ],
   "source": [
    "valid_gen = batches_from_sparse_array(xvalid_cwtv_cctv_glove, yvalid, batch_size=batch_size)\n",
    "steps = math.ceil(xvalid_cwtv_cctv_glove.shape[0]/batch_size)\n",
    "valid_predictions_dl_1 = dl_model_1.predict_generator(valid_gen, steps=steps)\n",
    "\n",
    "print_metrics(yvalid, valid_predictions_dl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.54000000000000004, 0.0, 0.45999999999999996)\n",
      "logloss: 0.386, accuracy: 0.859 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0)\n",
    "for i in tqdm(np.arange(0,1,0.02)):\n",
    "    for j in np.arange(0,1-i,0.02):\n",
    "        for k in np.arange(0,1-i-j,0.02):\n",
    "            predictions_ensamble2 = (\n",
    "                i*valid_predictions_dl_1 + \n",
    "                j*predictions_multi_2 + \n",
    "                k*predictions_comb + \n",
    "                (1-i-j-k)*predictions_nbtfidf\n",
    "            )\n",
    "            \n",
    "            loss = multiclass_logloss(yvalid, predictions_ensamble2)\n",
    "            if best_loss > loss: \n",
    "                best_loss = loss\n",
    "                best_params = (i,j,k,1-i-j-k)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble2 = (\n",
    "                best_params[0]*valid_predictions_dl_1 + \n",
    "                best_params[1]*predictions_multi_2 + \n",
    "                best_params[2]*predictions_comb + \n",
    "                best_params[3]*predictions_nbtfidf\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = batches_from_sparse_array(xtest_cwtv_cctv_glove, np.zeros(xtest_cwtv_cctv_glove.shape[0]), batch_size=batch_size)\n",
    "steps = math.ceil(xtest_cwtv_cctv_glove.shape[0]/batch_size)\n",
    "test_predictions_dl_1 = dl_model_1.predict_generator(test_gen, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble2 = (\n",
    "                best_params[0]*test_predictions_dl_1 + \n",
    "                best_params[1]*test_predictions_multi2 + \n",
    "                best_params[2]*test_predictions_comb + \n",
    "                best_params[3]*test_predictions_nbtfidf\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_3 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble2, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_3.to_csv('/data/spooky_author/submition_3.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence to vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.778, accuracy: 0.683 \n"
     ]
    }
   ],
   "source": [
    "sen2vec_lr = LogisticRegression(C=1.0)\n",
    "sen2vec_lr.fit(xvtrain, ytrain)\n",
    "predictions_sen2vec = sen2vec_lr.predict_proba(xvvalid)\n",
    "\n",
    "print_metrics(yvalid, predictions_sen2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv_s2v_glove = hstack((xtrain_glove,xvtrain,xtrain_cwtv,xtrain_cctv))\n",
    "xvalid_cwtv_cctv_s2v_glove = hstack((xvalid_glove,xvvalid,xvalid_cwtv,xvalid_cctv))\n",
    "xtest_cwtv_cctv_s2v_glove = hstack((xtest_glove,xvtest,xtest_cwtv,xtest_cctv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.497, accuracy: 0.824 \n"
     ]
    }
   ],
   "source": [
    "multi_lr_3 = LogisticRegression(C=1.0)\n",
    "multi_lr_3.fit(xtrain_cwtv_cctv_s2v_glove, ytrain)\n",
    "predictions_multi_3 = multi_lr_3.predict_proba(xvalid_cwtv_cctv_s2v_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_multi_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.30000000000000004, 0.0, 0.45000000000000001, 0.24999999999999994)\n",
      "logloss: 0.385, accuracy: 0.858 \n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0)\n",
    "step = 0.05\n",
    "for i in np.arange(0,0.1,step):\n",
    "    for j in np.arange(0,1-i,step):\n",
    "        for k in np.arange(0,1-i-j,step):\n",
    "            for l in np.arange(0,1-i-j-k, step):\n",
    "                predictions_ensamble3 = (\n",
    "                    i*valid_predictions_dl_1 + \n",
    "                    j*predictions_multi_2 + \n",
    "                    k*predictions_comb + \n",
    "                    l*predictions_nbtfidf + \n",
    "                    (1-i-j-k-l)*predictions_multi_3\n",
    "                )\n",
    "\n",
    "                loss = multiclass_logloss(yvalid, predictions_ensamble3)\n",
    "                if best_loss > loss: \n",
    "                    best_loss = loss\n",
    "                    best_params = (i,j,k,l,1-i-j-k-l)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble3 = (\n",
    "                best_params[0]*valid_predictions_dl_1 + \n",
    "                best_params[1]*predictions_multi_2 + \n",
    "                best_params[2]*predictions_comb + \n",
    "                best_params[3]*predictions_nbtfidf +\n",
    "                best_params[4]*predictions_multi_3\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit fourth model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_multi_3 = multi_lr_3.predict_proba(xtest_cwtv_cctv_s2v_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble3 = (\n",
    "                best_params[0]*test_predictions_dl_1 + \n",
    "                best_params[1]*test_predictions_multi2 + \n",
    "                best_params[2]*test_predictions_comb + \n",
    "                best_params[3]*test_predictions_nbtfidf + \n",
    "                best_params[4]*test_predictions_multi_3\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_4 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble3, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_4.to_csv('/data/spooky_author/submition_4.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyvalid = np.argmax(predictions_ensamble2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[696,  30,  64],\n",
       "       [ 57, 474,  33],\n",
       "       [ 68,  25, 511]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(yvalid,pyvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9c7e74c8d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c370830b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(confusion_matrix(yvalid,pyvalid), cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lens = list(map(lambda x: len(x),xvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c7e7297b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = plt.hist(line_lens, bins=10, range=(0, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_for_len_groups(preds,keys = [30,50,100,150,200,250,300,301]):\n",
    "    \n",
    "\n",
    "    ygrouped = {k:[] for k in keys}\n",
    "    pred_grouped = {k:[] for k in keys}\n",
    "\n",
    "    def get_bin(v):\n",
    "        for i in keys:\n",
    "            if i > v:\n",
    "                return i\n",
    "        return keys[-1]\n",
    "\n",
    "    for i in range(len(yvalid)):\n",
    "        b = get_bin(line_lens[i])\n",
    "        ygrouped[b].append(yvalid[i])\n",
    "        pred_grouped[b].append(preds[i])\n",
    "\n",
    "    for k in keys:\n",
    "        print('Less than', k, 'characters', end=' > ')\n",
    "        print_metrics(np.array(ygrouped[k]),np.array(pred_grouped[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less than 30 characters > logloss: 0.666, accuracy: 0.739 \n",
      "Less than 50 characters > logloss: 0.594, accuracy: 0.783 \n",
      "Less than 100 characters > logloss: 0.522, accuracy: 0.797 \n",
      "Less than 150 characters > logloss: 0.401, accuracy: 0.857 \n",
      "Less than 200 characters > logloss: 0.258, accuracy: 0.926 \n",
      "Less than 250 characters > logloss: 0.297, accuracy: 0.891 \n",
      "Less than 300 characters > logloss: 0.209, accuracy: 0.926 \n",
      "Less than 301 characters > logloss: 0.178, accuracy: 0.915 \n"
     ]
    }
   ],
   "source": [
    "print_metrics_for_len_groups(predictions_ensamble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less than 150 characters > logloss: 0.485, accuracy: 0.818 \n",
      "Less than 151 characters > logloss: 0.247, accuracy: 0.915 \n"
     ]
    }
   ],
   "source": [
    "print_metrics_for_len_groups(predictions_ensamble2, [150,151])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf weighted glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfvs = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1,1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfvs.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfvs =  tfvs.transform(xtrain) \n",
    "xvalid_tfvs = tfvs.transform(xvalid)\n",
    "xtest_tfvs = tfvs.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvs_vocab = tfvs.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13003"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfvs.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec_tfidf(s,tfidf):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        if w in tfvs_vocab:\n",
    "            weight = tfidf[0,tfvs_vocab[w]]\n",
    "        else:\n",
    "            weight = 0.01\n",
    "        try:\n",
    "            M.append(embeddings_index[w]*weight)\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    l = np.sqrt((v ** 2).sum())\n",
    "    \n",
    "    if type(v) != np.ndarray or l < 0.0000001:\n",
    "        return np.zeros(300)\n",
    "    return v / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_weighted_glove = np.array([sent2vec_tfidf(x,xtrain_tfvs[i]) for i,x in enumerate(xtrain)])\n",
    "xvalid_weighted_glove = np.array([sent2vec_tfidf(x,xvalid_tfvs[i]) for i,x in enumerate(xvalid)])\n",
    "xtest_weighted_glove = np.array([sent2vec_tfidf(x,xtest_tfvs[i]) for i,x in enumerate(xtest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.760, accuracy: 0.682 \n"
     ]
    }
   ],
   "source": [
    "wglove_lr = LogisticRegression(C=1.0)\n",
    "wglove_lr.fit(xtrain_weighted_glove, ytrain)\n",
    "predictions_wglove = wglove_lr.predict_proba(xvalid_weighted_glove)\n",
    "\n",
    "print_metrics(yvalid, predictions_wglove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cwtv_cctv_s2v_wglove = hstack((xtrain_weighted_glove,xtrain_glove,xvtrain,xtrain_cwtv,xtrain_cctv))\n",
    "xvalid_cwtv_cctv_s2v_wglove = hstack((xvalid_weighted_glove,xvalid_glove,xvvalid,xvalid_cwtv,xvalid_cctv))\n",
    "xtest_cwtv_cctv_s2v_wglove = hstack((xtest_weighted_glove,xtest_glove,xvtest,xtest_cwtv,xtest_cctv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.491, accuracy: 0.826 \n"
     ]
    }
   ],
   "source": [
    "multi_lr_4 = LogisticRegression(C=1.0)\n",
    "multi_lr_4.fit(xtrain_cwtv_cctv_s2v_wglove, ytrain)\n",
    "predictions_multi_4 = multi_lr_4.predict_proba(xvalid_cwtv_cctv_s2v_wglove)\n",
    "\n",
    "print_metrics(yvalid, predictions_multi_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.30000000000000004, 0.0, 0.40000000000000002, 0.0, 0.29999999999999993)\n",
      "logloss: 0.384, accuracy: 0.854 \n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0)\n",
    "step = 0.1\n",
    "for i in np.arange(0,1,step):\n",
    "    for j in np.arange(0,1-i,step):\n",
    "        for k in np.arange(0,1-i-j,step):\n",
    "            for l in np.arange(0,1-i-j-k, step):\n",
    "                for m in np.arange(0,1-i-j-k-l, step):\n",
    "                    predictions_ensamble3 = (\n",
    "                        i*valid_predictions_dl_1 + \n",
    "                        j*predictions_multi_2 + \n",
    "                        k*predictions_comb + \n",
    "                        l*predictions_nbtfidf + \n",
    "                        m*predictions_multi_3 + \n",
    "                        (1-i-j-k-l-m)*predictions_multi_4\n",
    "                    )\n",
    "\n",
    "                    loss = multiclass_logloss(yvalid, predictions_ensamble3)\n",
    "                    if best_loss > loss: \n",
    "                        best_loss = loss\n",
    "                        best_params = (i,j,k,l,m,1-i-j-k-l-m)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble4 = (\n",
    "                best_params[0]*valid_predictions_dl_1 + \n",
    "                best_params[1]*predictions_multi_2 + \n",
    "                best_params[2]*predictions_comb + \n",
    "                best_params[3]*predictions_nbtfidf +\n",
    "                best_params[4]*predictions_multi_3 +\n",
    "                best_params[5]*predictions_multi_4\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit fifth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_multi_4 = multi_lr_4.predict_proba(xtest_cwtv_cctv_s2v_wglove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble4 = (\n",
    "                best_params[0]*test_predictions_dl_1 + \n",
    "                best_params[1]*test_predictions_multi2 + \n",
    "                best_params[2]*test_predictions_comb + \n",
    "                best_params[3]*test_predictions_nbtfidf + \n",
    "                best_params[4]*test_predictions_multi_3 + \n",
    "                best_params[5]*test_predictions_multi_4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_5 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble4, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_5.to_csv('/data/spooky_author/submition_5.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy good idea from kaggle\n",
    "https://www.kaggle.com/marcospinaci/0-335-log-loss-in-a-dozen-lines/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.354, accuracy: 0.868 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "models = [('MultiNB', MultinomialNB(alpha=0.03)),\n",
    "          ('Calibrated MultiNB', CalibratedClassifierCV(\n",
    "              MultinomialNB(alpha=0.03), method='isotonic')),\n",
    "          ('Calibrated BernoulliNB', CalibratedClassifierCV(\n",
    "              BernoulliNB(alpha=0.03), method='isotonic')),\n",
    "          ('Calibrated Huber', CalibratedClassifierCV(\n",
    "              SGDClassifier(loss='modified_huber', alpha=1e-4,\n",
    "                            max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "          ('Logit', LogisticRegression(C=30))]\n",
    "\n",
    "\n",
    "vectorizer=TfidfVectorizer(token_pattern=r'\\w{1,}', sublinear_tf=True, ngram_range=(1,2))\n",
    "clf = VotingClassifier(models, voting='soft', weights=[3,3,3,1,1])\n",
    "\n",
    "vectorizer.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "xtrain_tfv2 = vectorizer.transform(xtrain)\n",
    "xvalid_tfv2 = vectorizer.transform(xvalid)\n",
    "xtest_tfv2 = vectorizer.transform(xtest)\n",
    "\n",
    "\n",
    "clf.fit(xtrain_tfv2, ytrain)\n",
    "predictions_tfv2 = clf.predict_proba(xvalid_tfv2)\n",
    "\n",
    "print_metrics(yvalid, predictions_tfv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, 0.0, 0.80000000000000004, 0.19999999999999996)\n",
      "logloss: 0.347, accuracy: 0.878 \n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "best_params = (0,0,0,0,0)\n",
    "step = 0.1\n",
    "for i in np.arange(0,1+step,step):\n",
    "    for j in np.arange(0,1-i+step,step):\n",
    "        for k in np.arange(0,1-i-j+step,step):\n",
    "            for l in np.arange(0,1-i-j-k+step, step):\n",
    "                predictions_ensamble5 = (\n",
    "                    i*predictions_multi_2 + \n",
    "                    j*predictions_nbtfidf + \n",
    "                    k*predictions_multi_3 + \n",
    "                    l*predictions_tfv2 +\n",
    "                    (1-i-j-k-l)*predictions_multi_4\n",
    "                )\n",
    "\n",
    "                loss = multiclass_logloss(yvalid, predictions_ensamble5)\n",
    "                if best_loss > loss: \n",
    "                    best_loss = loss\n",
    "                    best_params = (i,j,k,l,1-i-j-k-l)\n",
    "\n",
    "print(best_params)\n",
    "predictions_ensamble5 = (\n",
    "                best_params[0]*predictions_multi_2 + \n",
    "                best_params[1]*predictions_nbtfidf +\n",
    "                best_params[2]*predictions_multi_3 +\n",
    "                best_params[3]*predictions_tfv2 +\n",
    "                best_params[4]*predictions_multi_4\n",
    "            )\n",
    "\n",
    "print_metrics(yvalid, predictions_ensamble5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit sixth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_tfv2 = clf.predict_proba(xtest_tfv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ensamble5 = (\n",
    "                best_params[0]*test_predictions_multi2 + \n",
    "                best_params[1]*test_predictions_nbtfidf +\n",
    "                best_params[2]*test_predictions_multi_3 +\n",
    "                best_params[3]*test_predictions_tfv2 +\n",
    "                best_params[4]*test_predictions_multi_4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_6 = pd.concat([test['id'],pd.DataFrame(test_predictions_ensamble5, columns=lbl_enc.classes_)], axis=1)\n",
    "test_result_6.to_csv('/data/spooky_author/submition_6.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "- boost ensamble\n",
    "- attention model\n",
    "- conv network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
